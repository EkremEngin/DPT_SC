#!/usr/bin/env ts-node
/**
 * S3-Compatible Off-Site Backup Sync
 * 
 * Encrypts and uploads local backups to S3-compatible object storage.
 * 
 * Features:
 * - AES-256-CBC encryption with PBKDF2 key derivation
 * - SHA-256 checksum verification
 * - Retries with exponential backoff
 * - Versioned upload metadata
 * 
 * Usage:
 *   npm run backup:offsite -- [local-file]
 *   npm run backup:offsite-all  # Upload all pending backups
 */

import * as fs from 'fs';
import * as path from 'path';
import * as crypto from 'crypto';
import { execSync } from 'child_process';

// ============================================================================
// Configuration
// ============================================================================

interface S3Config {
  endpoint: string;
  bucket: string;
  region: string;
  accessKeyId: string;
  secretAccessKey: string;
  prefix: string;
  versioning: boolean;
}

interface EncryptionConfig {
  algorithm: 'aes-256-cbc';
  keyDerivation: 'pbkdf2';
  iterations: number;
  saltLength: number;
  ivLength: number;
  keyLength: number;
  hashAlgorithm: 'sha256';
}

const S3_CONFIG: S3Config = {
  endpoint: process.env.S3_ENDPOINT || 'https://s3.amazonaws.com',
  bucket: process.env.S3_BUCKET_NAME || 'dpt-local-backups',
  region: process.env.S3_REGION || 'us-east-1',
  accessKeyId: process.env.AWS_ACCESS_KEY_ID || '',
  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY || '',
  prefix: process.env.S3_PREFIX || 'dpt-local-backups/',
  versioning: true
};

const ENCRYPTION_CONFIG: EncryptionConfig = {
  algorithm: 'aes-256-cbc',
  keyDerivation: 'pbkdf2',
  iterations: 100000,
  saltLength: 32,
  ivLength: 16,
  keyLength: 32,
  hashAlgorithm: 'sha256'
};

const RETRY_CONFIG = {
  maxAttempts: 3,
  baseDelayMs: 5000,
  maxDelayMs: 45000
};

// ============================================================================
// Encryption
// ============================================================================

interface EncryptionResult {
  encryptedPath: string;
  saltHex: string;
  ivHex: string;
  checksumSHA256: string;
  originalSize: number;
  encryptedSize: number;
}

/**
 * Encrypts a file using AES-256-CBC with PBKDF2 key derivation.
 * 
 * File format: [salt (32B)] + [IV (16B)] + [ciphertext]
 * Extension: .sql.gz.enc
 */
async function encryptFile(filePath: string): Promise<EncryptionResult> {
  const encryptionKey = process.env.BACKUP_ENCRYPTION_KEY;
  
  if (!encryptionKey) {
    throw new Error('BACKUP_ENCRYPTION_KEY environment variable is required');
  }
  
  if (encryptionKey.length < 32) {
    throw new Error('BACKUP_ENCRYPTION_KEY must be at least 32 characters');
  }
  
  console.log(`üîí Encrypting: ${path.basename(filePath)}`);
  
  // Read plaintext file
  const plaintext = fs.readFileSync(filePath);
  const originalSize = plaintext.length;
  
  // Generate random salt and IV
  const salt = crypto.randomBytes(ENCRYPTION_CONFIG.saltLength);
  const iv = crypto.randomBytes(ENCRYPTION_CONFIG.ivLength);
  
  // Derive encryption key using PBKDF2
  const derivedKey = crypto.pbkdf2Sync(
    encryptionKey,
    salt,
    ENCRYPTION_CONFIG.iterations,
    ENCRYPTION_CONFIG.keyLength,
    ENCRYPTION_CONFIG.hashAlgorithm
  );
  
  // Create cipher
  const cipher = crypto.createCipheriv(
    ENCRYPTION_CONFIG.algorithm,
    derivedKey,
    iv
  );
  
  // Encrypt data
  const ciphertext = Buffer.concat([
    cipher.update(plaintext),
    cipher.final()
  ]);
  
  // Write encrypted file: salt + iv + ciphertext
  const encryptedPath = filePath + '.enc';
  const encryptedBuffer = Buffer.concat([salt, iv, ciphertext]);
  fs.writeFileSync(encryptedPath, encryptedBuffer);
  
  // Calculate SHA-256 checksum of encrypted file
  const checksumSHA256 = crypto
    .createHash(ENCRYPTION_CONFIG.hashAlgorithm)
    .update(encryptedBuffer)
    .digest('hex');
  
  console.log(`‚úÖ Encryption complete:`);
  console.log(`   Original size: ${(originalSize / 1024 / 1024).toFixed(2)} MB`);
  console.log(`   Encrypted size: ${(encryptedBuffer.length / 1024 / 1024).toFixed(2)} MB`);
  console.log(`   Checksum: ${checksumSHA256}`);
  
  return {
    encryptedPath,
    saltHex: salt.toString('hex'),
    ivHex: iv.toString('hex'),
    checksumSHA256,
    originalSize,
    encryptedSize: encryptedBuffer.length
  };
}

// ============================================================================
// S3 Upload
// ============================================================================

interface UploadMetadata {
  originalFilename: string;
  encryptionAlgorithm: string;
  saltHex: string;
  ivHex: string;
  checksumSHA256: string;
  uploadTimestamp: string;
  unencryptedSizeMB: number;
  encryptedSizeMB: number;
}

interface UploadResult {
  success: boolean;
  s3Key: string;
  versionId?: string;
  error?: string;
}

/**
 * Uploads a file to S3 using AWS CLI (more reliable than native implementations).
 */
async function uploadToS3(
  filePath: string,
  metadata: UploadMetadata
): Promise<UploadResult> {
  const s3Key = `${S3_CONFIG.prefix}${path.basename(filePath)}`;
  const s3Uri = `s3://${S3_CONFIG.bucket}/${s3Key}`;
  
  // Build metadata arguments
  const metadataArgs = Object.entries(metadata)
    .flatMap(([key, value]) => ['--metadata', `${key}=${value}`]);
  
  // Build command
  const args = [
    's3', 'cp',
    filePath,
    s3Uri,
    '--region', S3_CONFIG.region,
    '--endpoint-url', S3_CONFIG.endpoint,
    ...metadataArgs,
    '--no-progress'
  ];
  
  console.log(`‚òÅÔ∏è  Uploading to S3: ${s3Key}`);
  
  // Set environment for AWS CLI
  const env = {
    ...process.env,
    AWS_ACCESS_KEY_ID: S3_CONFIG.accessKeyId,
    AWS_SECRET_ACCESS_KEY: S3_CONFIG.secretAccessKey,
    AWS_DEFAULT_REGION: S3_CONFIG.region
  };
  
  try {
    const output = execSync(`aws ${args.join(' ')}`, {
      env,
      encoding: 'utf-8',
      stdio: ['pipe', 'pipe', 'pipe']
    });
    
    // Extract version ID if available (requires --versioning-config on bucket)
    let versionId: string | undefined;
    
    // Verify upload by checking file size
    const verifyArgs = [
      's3api', 'head-object',
      '--bucket', S3_CONFIG.bucket,
      '--key', s3Key,
      '--region', S3_CONFIG.region,
      '--endpoint-url', S3_CONFIG.endpoint
    ];
    
    try {
      const headOutput = execSync(`aws ${verifyArgs.join(' ')}`, {
        env,
        encoding: 'utf-8'
      });
      console.log(`‚úÖ Upload verified via head-object`);
    } catch (verifyError) {
      // head-object might fail with certain S3-compatible services
      console.log(`‚ö†Ô∏è  Could not verify upload, but no errors reported`);
    }
    
    console.log(`‚úÖ Upload successful: ${s3Uri}`);
    
    return {
      success: true,
      s3Key,
      versionId
    };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`‚ùå Upload failed: ${errorMessage}`);
    return {
      success: false,
      s3Key,
      error: errorMessage
    };
  }
}

/**
 * Uploads with retry logic and exponential backoff.
 */
async function uploadWithRetry(
  filePath: string,
  metadata: UploadMetadata
): Promise<UploadResult> {
  for (let attempt = 1; attempt <= RETRY_CONFIG.maxAttempts; attempt++) {
    console.log(`üì§ Upload attempt ${attempt}/${RETRY_CONFIG.maxAttempts}`);
    
    const result = await uploadToS3(filePath, metadata);
    
    if (result.success) {
      return result;
    }
    
    if (attempt < RETRY_CONFIG.maxAttempts) {
      const delay = Math.min(
        RETRY_CONFIG.baseDelayMs * Math.pow(2, attempt - 1),
        RETRY_CONFIG.maxDelayMs
      );
      console.log(`‚è≥ Retrying in ${delay / 1000} seconds...`);
      await sleep(delay);
    }
  }
  
  return {
    success: false,
    s3Key: '',
    error: `Failed after ${RETRY_CONFIG.maxAttempts} attempts`
  };
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// ============================================================================
// Main Logic
// ============================================================================

interface SyncResult {
  localFile: string;
  encrypted?: string;
  s3Key?: string;
  success: boolean;
  error?: string;
  durationSec: number;
}

/**
 * Syncs a single backup file to S3.
 */
async function syncBackup(localFile: string): Promise<SyncResult> {
  const startTime = Date.now();
  
  try {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`üì¶ Processing: ${path.basename(localFile)}`);
    console.log(`${'='.repeat(60)}`);
    
    // Check if file exists
    if (!fs.existsSync(localFile)) {
      throw new Error(`File not found: ${localFile}`);
    }
    
    // Skip if already encrypted and uploaded
    const encryptedFile = localFile + '.enc';
    
    let encryptionResult: EncryptionResult;
    
    // Check if encrypted version exists and is valid
    if (fs.existsSync(encryptedFile)) {
      console.log(`‚ÑπÔ∏è  Encrypted file exists, reusing...`);
      const encryptedBuffer = fs.readFileSync(encryptedFile);
      
      // Extract salt and IV for verification
      const salt = encryptedBuffer.subarray(0, ENCRYPTION_CONFIG.saltLength);
      const iv = encryptedBuffer.subarray(
        ENCRYPTION_CONFIG.saltLength,
        ENCRYPTION_CONFIG.saltLength + ENCRYPTION_CONFIG.ivLength
      );
      
      encryptionResult = {
        encryptedPath: encryptedFile,
        saltHex: salt.toString('hex'),
        ivHex: iv.toString('hex'),
        checksumSHA256: crypto
          .createHash('sha256')
          .update(encryptedBuffer)
          .digest('hex'),
        originalSize: fs.statSync(localFile).size,
        encryptedSize: encryptedBuffer.length
      };
    } else {
      // Encrypt the file
      encryptionResult = await encryptFile(localFile);
    }
    
    // Prepare upload metadata
    const uploadMetadata: UploadMetadata = {
      originalFilename: path.basename(localFile),
      encryptionAlgorithm: ENCRYPTION_CONFIG.algorithm,
      saltHex: encryptionResult.saltHex,
      ivHex: encryptionResult.ivHex,
      checksumSHA256: encryptionResult.checksumSHA256,
      uploadTimestamp: new Date().toISOString(),
      unencryptedSizeMB: Number((encryptionResult.originalSize / 1024 / 1024).toFixed(2)),
      encryptedSizeMB: Number((encryptionResult.encryptedSize / 1024 / 1024).toFixed(2))
    };
    
    // Upload to S3
    const uploadResult = await uploadWithRetry(
      encryptionResult.encryptedPath,
      uploadMetadata
    );
    
    const duration = (Date.now() - startTime) / 1000;
    
    if (uploadResult.success) {
      console.log(`\n‚úÖ Sync complete in ${duration.toFixed(2)}s`);
      console.log(`   S3 Key: ${uploadResult.s3Key}`);
      console.log(`   Version: ${uploadResult.versionId || 'N/A'}`);
    } else {
      console.error(`\n‚ùå Sync failed: ${uploadResult.error}`);
    }
    
    return {
      localFile,
      encrypted: encryptionResult.encryptedPath,
      s3Key: uploadResult.s3Key,
      success: uploadResult.success,
      error: uploadResult.error,
      durationSec: duration
    };
  } catch (error) {
    const duration = (Date.now() - startTime) / 1000;
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`\n‚ùå Error processing ${localFile}: ${errorMessage}`);
    
    return {
      localFile,
      success: false,
      error: errorMessage,
      durationSec: duration
    };
  }
}

/**
 * Syncs all pending backup files.
 */
async function syncAllBackups(): Promise<void> {
  const backupDir = process.env.BACKUP_DIR || './backups';
  
  if (!fs.existsSync(backupDir)) {
    console.log(`‚ÑπÔ∏è  Backup directory does not exist: ${backupDir}`);
    return;
  }
  
  const files = fs.readdirSync(backupDir)
    .filter(f => f.endsWith('.sql') || f.endsWith('.sql.gz'))
    .filter(f => !f.endsWith('.enc'))
    .map(f => path.join(backupDir, f))
    .sort((a, b) => {
      // Process newest first
      return fs.statSync(b).mtimeMs - fs.statSync(a).mtimeMs;
    });
  
  if (files.length === 0) {
    console.log('‚ÑπÔ∏è  No backup files to sync');
    return;
  }
  
  console.log(`üìÇ Found ${files.length} backup file(s) to sync`);
  
  const results: SyncResult[] = [];
  
  for (const file of files) {
    const result = await syncBackup(file);
    results.push(result);
  }
  
  // Summary
  console.log(`\n${'='.repeat(60)}`);
  console.log('üìä SYNC SUMMARY');
  console.log(`${'='.repeat(60)}`);
  
  const successful = results.filter(r => r.success).length;
  const failed = results.filter(r => !r.success).length;
  const totalDuration = results.reduce((sum, r) => sum + r.durationSec, 0);
  
  console.log(`Total files: ${results.length}`);
  console.log(`Successful: ${successful}`);
  console.log(`Failed: ${failed}`);
  console.log(`Total duration: ${totalDuration.toFixed(2)}s`);
  
  if (failed > 0) {
    console.log(`\n‚ùå Failed uploads:`);
    results
      .filter(r => !r.success)
      .forEach(r => {
        console.log(`   - ${path.basename(r.localFile)}: ${r.error}`);
      });
  }
}

// ============================================================================
// CLI
// ============================================================================

async function main() {
  const args = process.argv.slice(2);
  
  // Validate configuration
  const errors: string[] = [];
  
  if (!S3_CONFIG.accessKeyId) {
    errors.push('AWS_ACCESS_KEY_ID environment variable is required');
  }
  if (!S3_CONFIG.secretAccessKey) {
    errors.push('AWS_SECRET_ACCESS_KEY environment variable is required');
  }
  if (!process.env.BACKUP_ENCRYPTION_KEY) {
    errors.push('BACKUP_ENCRYPTION_KEY environment variable is required');
  }
  
  if (errors.length > 0) {
    console.error('‚ùå Configuration errors:');
    errors.forEach(e => console.error(`   ${e}`));
    process.exit(1);
  }
  
  console.log('üîê S3 Off-Site Backup Sync');
  console.log(`   Bucket: ${S3_CONFIG.bucket}`);
  console.log(`   Region: ${S3_CONFIG.region}`);
  console.log(`   Endpoint: ${S3_CONFIG.endpoint}`);
  console.log(`   Encryption: ${ENCRYPTION_CONFIG.algorithm} + ${ENCRYPTION_CONFIG.keyDerivation}`);
  console.log('');
  
  if (args.includes('--all') || args.length === 0) {
    await syncAllBackups();
  } else {
    const targetFile = args[0];
    if (!fs.existsSync(targetFile)) {
      console.error(`‚ùå File not found: ${targetFile}`);
      process.exit(1);
    }
    const result = await syncBackup(targetFile);
    
    if (!result.success) {
      process.exit(1);
    }
  }
}

main().catch(err => {
  console.error('Fatal error:', err);
  process.exit(1);
});
